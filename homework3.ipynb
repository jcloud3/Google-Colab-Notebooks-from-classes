{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"homework3.ipynb","provenance":[],"authorship_tag":"ABX9TyP7Yq8VwQu2XI5MnL6AbnbD"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"9L7AX_8zJR0r","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":238},"executionInfo":{"status":"ok","timestamp":1592188059835,"user_tz":300,"elapsed":6181,"user":{"displayName":"John Cloud","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdyfjLOszF3hOprvf6iyRHWHgP9QYBUjtQcUeyDA=s64","userId":"14413912309111282643"}},"outputId":"b93c3da1-2448-48b2-ca54-6cb636ca18d4"},"source":["import gym\n","import numpy as np\n","import time, pickle, os\n","import math\n","\n","array = np.array(list(\"SFFHFFHFG\"))\n","gamma = 0.95\n","lr_rate = 0.28\n","epsilon = 0.18\n","total_episodes = 28392\n","seedval = 679387\n","\n","amap = np.reshape(array,(3,3))\n","env = gym.make(\"FrozenLake-v0\", desc=amap).unwrapped\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","Q = np.zeros((env.observation_space.n, env.action_space.n))\n","#Q = np.full((env.observation_space.n, env.action_space.n),.5)\n","#Q[4]=[0,0,0,0]\n","#Q[15]=[0,0,0,0]\n","randomcount1 = 0\n","randomcount2 = 0\n","\n","def choose_action(state):\n","  action=0\n","  global randomcount1\n","  global randomcount2\n","  randomcount1 += 1\n","  if np.random.random() < epsilon:\n","    action = np.random.randint(0,4)\n","    \n","  else:\n","    action = np.argmax(Q[state, :])\n","  #print(action)\n","  return action\n","\n","def learn(state, state2, reward, action, action2):\n","\tpredict = Q[state, action]\n","\ttarget = reward + gamma * Q[state2, action2]\n","\tQ[state, action] = Q[state, action] + lr_rate * (target - predict)\n","\n","# Start\n","\n","\n","rewards=0\n","env.seed(seedval)\n","np.random.seed(seedval)\n","for episode in range(total_episodes):\n","  t = 0\n","  state = env.reset()\n","  action = choose_action(state)\n","  \n","\n","    \n","  while True:\n","    #env.render()\n","    #print(\"hello\")\n","\n","    state2, reward, done, info = env.step(action)\n","\n","    action2 = choose_action(state2)\n","\n","    learn(state, state2, reward, action, action2)\n","\n","    state = state2\n","    action = action2\n","\n","    t += 1\n","    rewards+=1\n","\n","    if done:\n","      break\n","  # epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode) \n","  # os.system('clear')\n","\t\t#time.sleep(0.1)\n","print(randomcount1)\n","print(randomcount2)\n","print (\"Score over time: \", rewards/total_episodes)\n","print(Q)\n","directions = np.argmax(Q, axis=1)\n","\n","truedirections = \"\"\n","for x in directions:\n","  if x == 0:\n","    truedirections = truedirections + \"<\"\n","  elif x == 1:\n","    truedirections = truedirections +  \"v\"\n","  elif x == 2:\n","    truedirections = truedirections +  \">\"\n","  elif x == 3:\n","    truedirections = truedirections +  \"^\"\n","  \n","print(truedirections)\n","#LEFT = 0\n","#DOWN = 1\n","#RIGHT = 2\n","#UP = 3\n","\n","with open(\"frozenLake_qTable_sarsa.pkl\", 'wb') as f:\n","\tpickle.dump(Q, f)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["367527\n","0\n","Score over time:  11.94473795435334\n","[[0.28391435 0.34751067 0.18810607 0.50850809]\n"," [0.4844263  0.56356414 0.65805783 0.52092919]\n"," [0.58558793 0.68180198 0.75860394 0.5447635 ]\n"," [0.         0.         0.         0.        ]\n"," [0.28160909 0.49381102 0.79843586 0.48162295]\n"," [0.67066424 0.882724   0.73111634 0.61880155]\n"," [0.         0.         0.         0.        ]\n"," [0.18376372 0.70309841 0.96704602 0.57442859]\n"," [0.         0.         0.         0.        ]]\n","^>><>v<><\n"],"name":"stdout"}]}]}