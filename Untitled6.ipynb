{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled6.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOTeA2w4dzul8keQQ7wFGHj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"Q982kV7pX0HR","executionInfo":{"status":"ok","timestamp":1627329091472,"user_tz":300,"elapsed":2432,"user":{"displayName":"John Cloud","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdyfjLOszF3hOprvf6iyRHWHgP9QYBUjtQcUeyDA=s64","userId":"14413912309111282643"}}},"source":["from keras.preprocessing.sequence import pad_sequences"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JaoVH_JoW5Gc","executionInfo":{"status":"ok","timestamp":1627332138605,"user_tz":300,"elapsed":101,"user":{"displayName":"John Cloud","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdyfjLOszF3hOprvf6iyRHWHgP9QYBUjtQcUeyDA=s64","userId":"14413912309111282643"}},"outputId":"ae4580a9-7086-4d65-aa8c-5607be7d0e39"},"source":["\n","start = [0]\n","    \n","start_word = [[start, 0.0]]\n","print(start_word)\n","print(len(start_word[0][0]))\n","max_length = 5\n","for s in start_word:\n","  print(s)\n","  sequence  = pad_sequences([s[0]], maxlen=max_length).reshape((1,max_length))\n","  print(sequence)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["[[[0], 0.0]]\n","1\n","[[0], 0.0]\n","[[0 0 0 0 0]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uFSRFemjWioW"},"source":["\n","\n","start = [0]\n","    \n","start_word = [[start, 0.0]]\n","\n","while len(start_word[0][0]) < max_length:\n","    temp = []\n","    for s in start_word:\n","        sequence  = pad_sequences([s[0]], maxlen=max_length).reshape((1,max_length)) #sequence of most probable words \n","                                                                                      # based on the previous steps\n","        preds = model.predict([pic_fe.reshape(1,2048), sequence])\n","        word_preds = np.argsort(preds[0])[-K_beams:] # sort predictions based on the probability, then take the last\n","                                                      # K_beams items. words with the most probs\n","        # Getting the top <K_beams>(n) predictions and creating a \n","        # new list so as to put them via the model again\n","        for w in word_preds:\n","            \n","            next_cap, prob = s[0][:], s[1]\n","            next_cap.append(w)\n","            if log:\n","                prob += np.log(preds[0][w]) # assign a probability to each K words4\n","            else:\n","                prob += preds[0][w]\n","            temp.append([next_cap, prob])\n","    start_word = temp\n","    # Sorting according to the probabilities\n","    start_word = sorted(start_word, reverse=False, key=lambda l: l[1])\n","\n","    # Getting the top words\n","    start_word = start_word[-K_beams:]\n","\n","start_word = start_word[-1][0]\n","captions_ = [ixtoword[i] for i in start_word]\n","\n","final_caption = []\n","\n","for i in captions_:\n","    if i != end_token:\n","        final_caption.append(i)\n","    else:\n","        break\n","\n","final_caption = ' '.join(final_caption[1:])\n"],"execution_count":null,"outputs":[]}]}